---
title: Regression On Dummy Variables
author: Peter von Rohr
date: "`r Sys.Date()`"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# rmdhelp::show_knit_hook_call()
knitr::knit_hooks$set(hook_convert_odg = rmdhelp::hook_convert_odg)
```

## Why

* Discrete valued predictor variables like `Breed`
* Assignment of numeric codes to different breeds creates dependencies between expected values of different breeds

\begin{align}
E(\text{BW Angus}) &= b_0 + b_1 \notag \\
E(\text{BW Limousin}) &= b_0 + 2b_1 \notag \\
E(\text{BW Simmental}) &= b_0 + 3b_1 \notag
\end{align}

* Only estimates are $b_0$ and $b_1$ 
* Usually unreasonable, with one exception


## Linear Regression in Genomic Analysis

* Regression on the number of positive alleles
* Estimate for slope $b_1$ corresponds to estimate of marker effect
* Review single-locus model from Quantitative Genetics


## Single Locus Model

```{r single-locus-model, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", out.width="100%"}
#rmdhelp::use_odg_graphic(ps_path = "odg/single-locus-model.odg")
knitr::include_graphics(path = "odg/single-locus-model.png")
```
 
* Assuming $d=0$ $\rightarrow$ genotypic value of $G_1G_2$ between homozygotes
* Shifting origin to genotypic value of $G_2G_2$


## Modified Single Locus Model

```{r mod-slm, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", out.width="100%"}
#rmdhelp::use_odg_graphic(ps_path = "odg/mod-slm.odg")
knitr::include_graphics(path = "odg/mod-slm.png")
```

* Transformation of regression on genotypes to regression on number of "positive" alleles ($G_1$)
* Relationships imposed by regression are meaningful


## Relationships

* Expected value for observation for a given genotype

\begin{align}
E(G_2G_2) &= b_0 + 0 * a_G \notag \\
E(G_1G_2) &= b_0 + 1 * a_G \notag \\
E(G_1G_1) &= b_0 + 2 * a_G \notag
\end{align}

* Differences

$$E(G_1G_2) - E(G_2G_2) =  E(G_1G_1) - E(G_1G_2) = a_G$$
$$E(G_1G_1) - E(G_2G_2) = 2a_G$$


## Example Dataset

* Exercise 3, Problem 1


## Regression On Dummy Variables

* Cases that are not like genomic data
* Example with breeds
* Discrete independent variables are called __Factors__ (e.g. Breed)
* Different values that a factor can take are called __Levels__
* Levels for our example factor `Breed` are: `Angus`, `Limousin` and `Simmental`


## Levels To Independent Variables

Use "separate" $x$-variable for each level, hence each of the breeds

```{r asm-flem-breed-var-assign, echo=FALSE, results='asis', message=FALSE, warning=FALSE}
s_data_dir <- file.path(here::here(), "docs", "data")
s_flem_path <- file.path(s_data_dir, "asm_bw_flem.csv")
tbl_flem <- readr::read_csv(file = s_flem_path)
tbl_breed_assign <- tibble::tibble(Breed = c(unique(sort(tbl_flem$Breed))),
                                   `Independent Variable`= c("$x_1$", "$x_2$", "$x_3$"))

knitr::kable(tbl_breed_assign,
             booktabs = TRUE,
             longtable = TRUE,
             escape = FALSE)
```



## Model

* Observation $y_{ij}$ stands for birth weight for animal $j$ in breed $i$

\begin{align} 
y_{11} &= b_0 + b_1 * 1 + b_2 * 0 + b_3 * 0 + e_{11} \notag \\
y_{12} &= b_0 + b_1 * 1 + b_2 * 0 + b_3 * 0 + e_{12} \notag \\
\cdots &= \cdots \notag \\
y_{33} &= b_0 + b_1 * 0 + b_2 * 0 + b_3 * 1 + e_{33} \notag
\end{align} 

* Sort animals according to breeds


## Matrix - Vector Notation

\begin{equation}
\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e} \notag
\end{equation}



## Models Not Of Full Rank

* Model

\begin{equation}
\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e} \notag
\end{equation}

* Least squares normal equations

\begin{equation}
\mathbf{X}^T\mathbf{X}\mathbf{b}^{(0)} = \mathbf{X}^T\mathbf{y} \notag
\end{equation}


## Solutions

* matrix $\mathbf{X}$ not of full rank, use Matrix::rankMatrix() to check
* $\mathbf{X}^T\mathbf{X}$ cannot be inverted
* solution

\begin{equation}
  \mathbf{b}^{(0)} = (\mathbf{X}^T\mathbf{X})^-\mathbf{X}^T\mathbf{y} \notag
\end{equation}

where $(\mathbf{X}^T\mathbf{X})^-$ stands for a __generalized inverse__


## Generalized Inverse

* matrix $\mathbf{G}$ is a generalized inverse of matrix $\mathbf{A}$, if 

$$\mathbf{AGA} = \mathbf{A}$$

$$(\mathbf{AGA})^T = \mathbf{A}^T$$

* Use `MASS::ginv()` in R


## Systems of Equations

* For a consistent system of equations

$$Ax = y$$

* Solution 

$$x = Gy$$
if $G$ is a generalized inverse of $A$. 

$$x = Gy$$
$$Ax = AGy$$
$$Ax = AGAx$$


## Non Uniqueness

* Solution $x = Gy$ is not unique

$$\tilde{\mathbf{x}} = \mathbf{Gy} + (\mathbf{GA} - \mathbf{I})\mathbf{z}$$
yields a different solution for an arbitrary vector $\mathbf{z}$

$$\mathbf{A}\tilde{\mathbf{x}} = \mathbf{A}\mathbf{Gy} + (\mathbf{A}\mathbf{GA} - \mathbf{A})\mathbf{z}$$

## Least Squares Normal Equations

* Instead of $Ax = y$, we have

\begin{equation}
\mathbf{X}^T\mathbf{X}\mathbf{b}^{(0)} = \mathbf{X}^T\mathbf{y} \notag
\end{equation}

* With generalized inverse $\mathbf{G}$ of $\mathbf{X}^T\mathbf{X}$

\begin{equation}
\mathbf{b}^{(0)} = \mathbf{G} \mathbf{X}^T\mathbf{y} \notag
\end{equation}

is a solution to the least squares normal equations


## Parameter Estimator

But $\mathbf{b}^{(0)}$ is not an estimator for the parameter $\mathbf{b}$, because

* it is not unique
* Expectation $E(\mathbf{b}^{(0)}) = E(\mathbf{G} \mathbf{X}^T\mathbf{y}) = \mathbf{G} \mathbf{X}^T \mathbf{Xb} \ne \mathbf{b}$


## Estimable Functions

```{r ex-estimable-function-data, echo=FALSE}
n_nr_rec_est_fun <- 6
tbl_est_fun <- tibble::tibble(Animal = c(1:n_nr_rec_est_fun),
                              Breed  = c(rep("Angus", 3), rep("Simmental", 2), "Limousin"),
                              Observation = c(16, 10, 19, 11, 13, 27))

knitr::kable(tbl_est_fun,
             booktabs = TRUE,
             longtable = FALSE,
             escape = FALSE)
```


## Model

$$\mathbf{y} = \mathbf{Xb} + \mathbf{e}$$

```{r, echo=FALSE, results='asis'}
# design matrix X
mat_x_est_fun <- matrix(c(1, 1, 0, 0,
                          1, 1, 0, 0,
                          1, 1, 0, 0,
                          1, 0, 1, 0,
                          1, 0, 1, 0,
                          1, 0, 0, 1), ncol = 4, byrow = TRUE)
# parameter vector b
vec_b <- c("\\mu", "\\alpha_1", "\\alpha_2", "\\alpha_3")
cat("$$\n")
cat(paste0(rmdhelp::bcolumn_vector(pvec = tbl_est_fun$Observation, ps_name = "\\mathbf{y}"), collapse = '\n'))
cat("\\text{, }")
cat(paste0(rmdhelp::bmatrix(pmat = mat_x_est_fun, ps_name = "\\mathbf{X}"), collapse = "\n"))
cat("\\text{ and }")
cat(paste0(rmdhelp::bcolumn_vector(pvec = vec_b, ps_name =  "\\mathbf{b}"), collapse = "\n"), "\n")
cat("$$\n")
```


## Normal Equations

$$ X^TXb^0 = X^Ty$$

```{r, echo=FALSE, results='asis'}
mat_xtx_est_fun <- crossprod(mat_x_est_fun)
mat_xty_est_fun <- crossprod(mat_x_est_fun, tbl_est_fun$Observation)
vec_b0 <- c("\\mu^0", "\\alpha_1^0", "\\alpha_2^0", "\\alpha_3^0")
cat("$$\n")
cat(paste0(rmdhelp::bmatrix(mat_xtx_est_fun), collapse = '\n'))
cat(paste0(rmdhelp::bcolumn_vector(pvec = vec_b0), collapse = '\n'))
cat(" = ")
cat(paste0(rmdhelp::bmatrix(pmat = mat_xty_est_fun), collapse = '\n'))
cat("$$\n")
```


## Solutions to Normal Equations

```{r est-fun-sol, echo=FALSE}
tbl_est_fun_sol <- tibble::tibble(`Elements of Solution` = c("$\\mu^0$", "$\\alpha_1^0$", "$\\alpha_2^0$", "$\\alpha_3^0$"),
                                  `$b_1^0$` = c(16, -1, -4, 11),
                                  `$b_2^0$` = c(14, 1, -2, 13),
                                  `$b_3^0$` = c(27, -12, -15, 0),
                                  `$b_4^0$` = c(-2982, 2997, 2994, 3009))
knitr::kable(tbl_est_fun_sol,
             booktabs = TRUE,
             longtable = FALSE,
             escape = FALSE)
```


## Functions of Solutions

```{r lin-fun-sol, echo=FALSE}
tbl_lin_fun_sol <- tibble::tibble(`Linear Function` = c("$\\alpha_1^0 - \\alpha_2^0$", "$\\mu^0 + \\alpha_1^0$", "$\\mu^0 + 1/2(\\alpha_2^0 + \\alpha_3^0)$"),
                                  `$b_1^0$` = c(3, 15, 19.5),
                                  `$b_2^0$` = c(3, 15, 19.5),
                                  `$b_3^0$` = c(3, 15, 19.5),
                                  `$b_4^0$` = c(3, 15, 19.5))
knitr::kable(tbl_lin_fun_sol,
             booktabs = TRUE,
             longtable = FALSE,
             escape = FALSE)
```

* $\alpha_1^0 - \alpha_2^0$: estimate of the difference between breed effects for Angus and Simmental
* $\mu^0 + \alpha_1^0$: estimate of the general mean plus the breed effect of Angus
* $\mu^0 + 1/2(\alpha_2^0 + \alpha_3^0)$: estimate of the general mean plus mean effect of breeds Simmental and Limousin


## Definition of Estimable Functions 

$$\mathbf{q}^T\mathbf{b} = \mathbf{t}^TE(\mathbf{y})$$

* Example

$$E(y_{1j}) = \mu + \alpha_1$$
with $\mathbf{t}^T = \left[\begin{array}{cccccc} 1 & 1 & 1 & 0 & 0 & 0 \end{array}\right]$ and $\mathbf{q}^T = \left[\begin{array}{cccc} 1 & 1 & 0 & 0 \end{array} \right]$

* Properties

$$\mathbf{q}^t = \mathbf{t}^T\mathbf{X}$$

* Test

$$\mathbf{q}^T\mathbf{H} = \mathbf{q}^T$$

with $\mathbf{H} = \mathbf{GX}^T\mathbf{X}$


