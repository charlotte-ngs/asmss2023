---
title: Linear Regression
author: Peter von Rohr
date: "2023-02-27"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# rmdhelp::show_knit_hook_call()
knitr::knit_hooks$set(hook_convert_odg = rmdhelp::hook_convert_odg)
```

## Goal

Assessment of relationship between 

- a given variable (response) and
- other measurements or observations (predictors) on the same animal


## Example

```{r tab-reg-bw-bc, echo=FALSE, results='asis'}
tbl_reg <- tibble::tibble(Animal = c(1:10),
                          `Breast Circumference` = c(176, 177, 178, 179, 179, 180, 181, 182,183, 184),
                          `Body Weight` = c(471, 463, 481, 470, 496, 491, 518, 511, 510, 541))
n_nr_obs <- nrow(tbl_reg)
knitr::kable(tbl_reg,
             booktabs = TRUE,
             longtable = TRUE,
             escape = FALSE)
```


## Diagram

```{r fig-reg-bw-bc, echo=FALSE, out.width="100%"}
library(ggplot2)
ggplot(tbl_reg, aes(x = `Breast Circumference`, y = `Body Weight`)) +
  geom_point(color = "blue") 
```


## Observations

- relationship between breast circumference and body weight: heavier animals tend to have larger values for breast circumference
- same relationship across whole range $\rightarrow$ __linear__ relationship


## Regression Model

- quantify relationship between body weight and breast circumference
- practical application: measure band for animals

```{r measure-band, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", out.width="100%"}
#rmdhelp::use_odg_graphic(ps_path = "odg/measure-band.odg")
knitr::include_graphics(path = "odg/measure-band.png")
```


## Model Building

- expected body weight ($E(y)$ in kg) based on an observed value of $x$ cm for breast circumference

\begin{equation}
E(y) = b_0 + b_1 * x \notag
\end{equation}

- $b_0$ and $b_1$ are unknown parameters of the model
- model is linear function of parameters $\rightarrow$ linear model


## Parameter Estimation

- How to find values for $b_0$ and $b_1$
- several techniques available: start with Least Squares


## Least Squares

```{r lsq-plot, echo=FALSE, hook_convert_odg=TRUE, fig_path="odg", out.width="100%"}
#rmdhelp::use_odg_graphic(ps_path = "odg/lsq-plot.odg")
knitr::include_graphics(path = "odg/lsq-plot.png")
```


## Estimators

Find values $\hat{b}_0$ and $\hat{b}_1$ such that 

\begin{equation}
\mathbf{e}^T\mathbf{e} = \sum_{i=1}^N e_i^2 = \sum_{i=1}^N \left[ y_i - E(e_i) \right]^2 = \sum_{i=1}^N \left[ y_i - b_0 - b_1*x_i \right]^2 \notag
\end{equation}

is minimal


## Minimization

\begin{align}
\frac{\partial\mathbf{e}^T\mathbf{e}}{\partial b_0} &= -2 \sum_{i=1}^N \left[y_i - b_0 - b_1x_i\right]  \notag \\
  &= -2\left[\sum_{i=1}^N y_i - Nb_0 - b_1\sum_{i=1}^N x_i\right] \notag
\end{align}

\begin{align}
\frac{\partial\mathbf{e}^T\mathbf{e}}{\partial b_1} &= -2 \sum_{i=1}^N x_i\left[y_i - b_0 - b_1x_i\right] \notag \\
  &= -2 \left[\sum_{i=1}^N x_iy_i - b_0 \sum_{i=1}^N x_i - b_1 \sum_{i=1}^N x_i^2 \right]\notag
\end{align}


## Notation

$$x. = \sum_{i=1}^N x_i$$
$$y. = \sum_{i=1}^N y_i$$
$$(x^2). = \sum_{i=1}^N x_i^2 $$ 
$$(xy). = \sum_{i=1}^N x_iy_i$$
$$\bar{x}. = {x. \over N}$$
$$\bar{y}. = {y. \over N}$$


## Solutions

\begin{equation}
\hat{b}_0 = \bar{y}. - \hat{b}_1\bar{x}.  \notag
\end{equation}

and

\begin{equation}
\hat{b}_1 = \frac{(xy). - N\bar{x}.\bar{y}.}{(x^2). - N\bar{x}.^2} \notag
\end{equation}


## The General Case
Height as additional observation 

```{r tab-bw-mult-reg, echo=FALSE}
tbl_bw_ext <- dplyr::bind_cols(tbl_reg, 
                               tibble::tibble(Height = c(161, 121, 157, 165, 136, 123, 163, 149, 143, 130)))
knitr::kable(tbl_bw_ext,
             booktabs = TRUE,
             longtable = TRUE,
             escape = FALSE)
```


## Extended Model
Height is taken as additional predictor variable

$$E(y) = b_0 + b_1x_1 + b_2x_2$$

$$y_i = b_0 + b_1x_{i1} + b_2x_{i2} + e_i$$

$\rightarrow$ additional parameter $b_2$


## Matrix-Vector Notation

```{r mat-vec-lm, echo=FALSE, results='asis'}
mat_X <- matrix(c("x_{10}", "x_{11}", "x_{12}",
                  "x_{20}", "x_{21}", "x_{22}",
                  ".", ".", ".",
                  ".", ".", ".",
                  ".", ".", ".",
                  "x_{N0}", "x_{N1}", "x_{N2}"), ncol = 3, byrow = TRUE)
vec_y <- c("y_1", "y_2", ".", ".", ".", "y_N")
vec_e <- c("e_1", "e_2", ".", ".", ".", "e_N")
vec_b <- c("b_0", "b_1", "b_2")

cat("$$\n")
cat(paste0(rmdhelp::bmatrix(pmat = mat_X, ps_name = "\\mathbf{X}"), collapse = ""), "\n")
cat(", \\ \n")
cat(paste0(rmdhelp::bcolumn_vector(pvec = vec_y, ps_name = "\\mathbf{y}")))
cat(", \\ \n")
cat(paste0(rmdhelp::bcolumn_vector(pvec = vec_e, ps_name = "\\mathbf{e}")))
cat("\\text{ and }\n")
cat(paste0(rmdhelp::bcolumn_vector(pvec = vec_b, ps_name = "\\mathbf{b}")))
cat("$$\n")
```

$$\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e} \text{, with } E(\mathbf{y}) = \mathbf{X}\mathbf{b}$$

## Parameter Estimate
Minimize

\begin{align}
\mathbf{e}^T\mathbf{e} &= \left[\mathbf{y} - E(\mathbf{y}) \right]^T\left[\mathbf{y} - E(\mathbf{y}) \right] \notag \\
                       &= \left[\mathbf{y} - \mathbf{Xb} \right]^T\left[\mathbf{y} - \mathbf{Xb} \right]\notag \\
                       &= \mathbf{y}^T\mathbf{y} - 2 \mathbf{b}^T\mathbf{X}^T\mathbf{y} + \mathbf{b}^T\mathbf{X}^T\mathbf{X}\mathbf{b} \notag
\end{align}

Compute the gradient $\frac{\partial \ \mathbf{e}^T\mathbf{e}}{\mathbf{b}}$, set it to $\mathbf{0}$ to get the normal equations

$$\mathbf{X}^T\mathbf{X}\hat{\mathbf{b}} = \mathbf{X}^T\mathbf{y}$$

## Solution
Provided $(\mathbf{X}^T\mathbf{X})$ can be inverted

$$\hat{\mathbf{b}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$


## Obtain Parameter Estimates in R

* Computations are tedious
* Use R builtin functions
* Assuming data is available in dataframe `tbl_reg` with columns `Body Weight` and `Breast Circumference`

\vspace{3ex}

```{r, echo=TRUE, eval=FALSE}
lm_bw_bc <- lm(`Body Weight` ~ `Breast Circumference`, 
               data = tbl_reg)
summary(lm_bw_bc)
```


## Multiple Linear Regression Model

\begin{equation}
\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e} \text{, with } E(\mathbf{y}) = \mathbf{X}\mathbf{b} \notag
\end{equation}

* General case with $k$ $x$-variables

```{r mat-vec-general-lm, echo=FALSE, results='asis'}
mat_X_gen <- matrix(c("x_{10}", "x_{11}", ".", "x_{1k}",
                  "x_{20}", "x_{21}", ".", "x_{2k}",
                  ".", ".", ".", ".",
                  ".", ".", ".", ".",
                  ".", ".", ".", ".",
                  "x_{N0}", "x_{N1}", ".", "x_{Nk}"), ncol = 4, byrow = TRUE)
vec_b_gen <- c("b_0", "b_1", ".", ".", "b_k")

cat("$$\n")
cat(paste0(rmdhelp::bmatrix(pmat = mat_X_gen, ps_name = "\\mathbf{X}"), collapse = ""), "\n")
cat(", \\ \n")
cat(paste0(rmdhelp::bcolumn_vector(pvec = vec_b_gen, ps_name = "\\mathbf{b}")))
cat("$$\n")
```


## Random Error Terms

* Properties of random error terms in vector $\mathbf{e}$

\begin{equation}
E(\mathbf{e}) = \mathbf{0}  \notag
\end{equation}

\begin{equation}
var(\mathbf{e}) = E\left[\mathbf{e} - E(\mathbf{e}) \right]\left[\mathbf{e} - E(\mathbf{e}) \right]^T 
= E(\mathbf{e}\mathbf{e}^T) = \sigma^2 \mathbf{I}_N \notag
\end{equation}


## Least Squares Estimates

\begin{align}
\mathbf{e}^T\mathbf{e} &= \left[\mathbf{y} - E(\mathbf{y}) \right]^T\left[\mathbf{y} - E(\mathbf{y}) \right] \notag \\
                       &= \left[\mathbf{y} - \mathbf{Xb} \right]^T\left[\mathbf{y} - \mathbf{Xb} \right]\notag \\
                       &= \mathbf{y}^T\mathbf{y} - 2 \mathbf{b}^T\mathbf{X}^T\mathbf{y} + \mathbf{b}^T\mathbf{X}^T\mathbf{X}\mathbf{b} \notag
\end{align}

* Setting

$$\frac{\partial \mathbf{e}^T\mathbf{e}}{\partial \mathbf{b}} = \mathbf{0}$$

* yields least squares normal equations

\begin{equation}
\mathbf{X}^T\mathbf{X}\hat{\mathbf{b}} = \mathbf{X}^T\mathbf{y} \notag
\end{equation}


## Solution for Least Squares Estimators


\begin{equation}
\hat{\mathbf{b}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \notag
\end{equation}

